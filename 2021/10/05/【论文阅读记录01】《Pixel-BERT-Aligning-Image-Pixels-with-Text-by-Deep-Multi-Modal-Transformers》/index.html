<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2004.00849 年份：2020 0.Abstract Pixel-BERT将图像像素与文本对齐，通过深层多模态Transformers在统一的end-to-end框架上来共同学习视觉和语言embedding。 多数VL任务都使用基于region的图像特征（BUTD啥的），而Pixel-BERT则直接从图像-语句对中建立图像像素和语义更为准">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读记录01】《Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers》">
<meta property="og:url" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2004.00849 年份：2020 0.Abstract Pixel-BERT将图像像素与文本对齐，通过深层多模态Transformers在统一的end-to-end框架上来共同学习视觉和语言embedding。 多数VL任务都使用基于region的图像特征（BUTD啥的），而Pixel-BERT则直接从图像-语句对中建立图像像素和语义更为准">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/1.png">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/2.png">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/3.png">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/4.png">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/5.png">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/6.png">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/7.png">
<meta property="og:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/8.png">
<meta property="article:published_time" content="2021-10-05T02:07:28.000Z">
<meta property="article:modified_time" content="2021-10-06T01:35:54.595Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/1.png">


<link rel="canonical" href="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/","path":"2021/10/05/【论文阅读记录01】《Pixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers》/","title":"【论文阅读记录01】《Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers》"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>【论文阅读记录01】《Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers》 | Hexo</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-Abstract"><span class="nav-number">1.</span> <span class="nav-text">0.Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">1.Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Related-Works"><span class="nav-number">3.</span> <span class="nav-text">2.Related Works</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Pre-training-Mechanism"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 Pre-training Mechanism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Visual-Feature-Embedding-in-Vision-and-Language-Tasks"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 Visual Feature Embedding in Vision and Language Tasks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Approach"><span class="nav-number">4.</span> <span class="nav-text">3.Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Revisit-Transformer"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Revisit Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Model-Architecture"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Model Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sentence-Feature-Embedding"><span class="nav-number">4.2.1.</span> <span class="nav-text">Sentence Feature Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Feature-Embedding"><span class="nav-number">4.2.2.</span> <span class="nav-text">Image Feature Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Modality-Module"><span class="nav-number">4.2.3.</span> <span class="nav-text">Cross-Modality Module</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Pre-Training"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 Pre-Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Masked-Language-Modeling"><span class="nav-number">4.3.1.</span> <span class="nav-text">Masked Language Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Text-Matching"><span class="nav-number">4.3.2.</span> <span class="nav-text">Image-Text Matching</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pixel-Random-Sampling"><span class="nav-number">4.3.3.</span> <span class="nav-text">Pixel Random Sampling</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Experiments"><span class="nav-number">5.</span> <span class="nav-text">4.Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Pre-training"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 Pre-training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Datasets"><span class="nav-number">5.1.1.</span> <span class="nav-text">Datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementation-Details"><span class="nav-number">5.1.2.</span> <span class="nav-text">Implementation Details</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Downstream-Tasks"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 Downstream Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Visual-Question-Answering"><span class="nav-number">5.2.1.</span> <span class="nav-text">Visual Question Answering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Natural-Language-for-Visual-Reasoning-for-Real"><span class="nav-number">5.2.2.</span> <span class="nav-text">Natural Language for Visual Reasoning for Real</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Text-Retrieval"><span class="nav-number">5.2.3.</span> <span class="nav-text">Image-Text Retrieval</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-Ablation-Study"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 Ablation Study</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-Visualization"><span class="nav-number">5.4.</span> <span class="nav-text">4.4 Visualization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Conclusion-and-Discussion"><span class="nav-number">6.</span> <span class="nav-text">5 Conclusion and Discussion</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【论文阅读记录01】《Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers》
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-10-05 10:07:28" itemprop="dateCreated datePublished" datetime="2021-10-05T10:07:28+08:00">2021-10-05</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-10-06 09:35:54" itemprop="dateModified" datetime="2021-10-06T09:35:54+08:00">2021-10-06</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><strong>论文链接</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.00849">https://arxiv.org/abs/2004.00849</a></p>
<p><strong>年份</strong>：2020</p>
<h1 id="0-Abstract"><a href="#0-Abstract" class="headerlink" title="0.Abstract"></a>0.Abstract</h1><ul>
<li>Pixel-BERT将图像像素与文本对齐，通过深层多模态Transformers在统一的end-to-end框架上来共同学习视觉和语言embedding。</li>
<li>多数VL任务都使用基于region的图像特征（BUTD啥的），而Pixel-BERT则直接从图像-语句对中建立图像像素和语义更为准确和周密的联系。Pixel-BERT在<strong>像素和文本级别对齐语义连接</strong>,解决了VL任务在特定任务的视觉表征上的限制。另一方面，减轻标注边界框的耗费，克服视觉任务语义标签和语言语义的不平衡。</li>
<li>在VG和MSCOCO数据集的图像文本对预训练一个end-to-end模型，使用一个随机像素采样的机制来提升视觉特征的鲁棒性，采用MLM和IMT作为预训练任务。</li>
<li>在VQA和NLVRT等下游任务上取得SOTA，且在VQA任务上单个模型上比当前SOTA涨点2.7。<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1></li>
<li>多模态预训练模型中，主要使用基于BERT的语言特征和基于region的视觉特征作为输入来共同学习。<blockquote>
<p>不同模态之间的语义隔阂一直被视为跨模态研究中的挑战。早期VL工作上（VQA或caption)使用在图像分类任务预训练的CNN模型抽取特征。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>再后来，VG数据集和BUTD的出现，使用目标检测模型（如Fast R-CNN）抽取的基于region的视觉特征能取得更好的表现。然而，基于region的视觉特征提取器是专门为如OD任务设计的，这会在语义理解上产生信息隔阂，且边界框的矩形形状在背景中会带来噪声信息，一些重要的视觉信息会丢失，像目标的形状、重叠边界框的对象的空间关系等。此外，特征表征能力还受限于给定种类，目标检测模型中也丢失了有关场景和情绪等更广泛语义的视觉信息。</p>
</blockquote>
<p><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/1.png" alt="Fig.1"></p>
<ul>
<li>在Fig.1中：A例子，让目标检测模型取得飞机的状态较为困难；B例子中，即使可以检测到地面和女孩，但因为她们的region重叠，之后fusion embedding模型对于给定边界框来判断实际空间联系就比较困难；C例子中，视觉特征只有长颈鹿，但很难去推理出这些动物的状态。</li>
<li><strong>在跨模态联合学习中使用基于region的视觉特征和语言embedding作为Transformer的输入受限在视觉特征所表示的视觉语义</strong>。因此，提出Pixel-BERT来跨出边界框，在VL learning利用图像的所有视觉特征。</li>
<li>Piexel BERT包含三个部分：<ol>
<li>visual encoder:一个CNN，将图片像素作为输入。</li>
<li>language encoder:基于BERT的word-level token embedding。</li>
<li>fusion model:多模态Transformer</li>
</ol>
</li>
<li><p>为了学习广义特征，首先在图像-语句对数据集上预训练模型。在预训练中采用两个预训练任务和一个预训练机制。在语言上，跟其他预训练工作一样采用MLM；在视觉和语言交互上，采取ITM去区分图像和语句对是否匹配。在视觉上，我们采取<strong>随机像素采样机制</strong>来弥补预测像素级特征的难度。随机像素采样机制提高视觉特征学习的稳健性且克服了过拟合问题。</p>
</li>
<li><p><strong>Contributions</strong>（总结一下上面的废话…）:</p>
</li>
</ul>
<ol>
<li>提出Pixel-BERT——由基于CNN的visualencoder和深层多模态Transformer组成，首先考虑使用自监督学习在像素和文本级别对齐视觉和语言语义。</li>
<li>我们以预训练方式使用模型，并提出了随机像素采样机制，以提高视觉表现学习的稳健性。</li>
<li>在包括VQA、Image-Text Retrieval和NLVR的任务上取得SOTA；特别地，我们的方法对比当前SOTA的表现上涨2.17点，比其larger model表现还高。</li>
</ol>
<h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2.Related Works"></a>2.Related Works</h1><h2 id="2-1-Pre-training-Mechanism"><a href="#2-1-Pre-training-Mechanism" class="headerlink" title="2.1 Pre-training Mechanism"></a>2.1 Pre-training Mechanism</h2><ul>
<li>基于网络框架，当前多模态预训练的方法分为两类：</li>
</ul>
<ol>
<li>基于Transformer的Two-stream神经网络，two-stream神经网络分别处理视觉和语言信息，然后用另外的Transformer层来融合它们。</li>
<li>Single-Stream神经网络。在检测边界框特征和语句embedding特征上使用BERT来学习双向联合分布。</li>
</ol>
<p>它们的差别在于训练方法、损失函数和数据集。<strong>Pixel-BERT属于Single-Stream，但visual embedding不同于其他方法。</strong></p>
<h2 id="2-2-Visual-Feature-Embedding-in-Vision-and-Language-Tasks"><a href="#2-2-Visual-Feature-Embedding-in-Vision-and-Language-Tasks" class="headerlink" title="2.2 Visual Feature Embedding in Vision and Language Tasks"></a>2.2 Visual Feature Embedding in Vision and Language Tasks</h2><ul>
<li>跟第一部分Introdution提到一样，用目标检测模型提取视觉特征的话，视觉语义受限于给定种类，然而语言特征包含更多的语义，造成不平衡。因此，我们将visual encoder学习网络组合成一个框架，并将源图像输入为视觉输入。</li>
</ul>
<h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3.Approach"></a>3.Approach</h1><p><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/2.png" alt="Fig.2"></p>
<h2 id="3-1-Revisit-Transformer"><a href="#3-1-Revisit-Transformer" class="headerlink" title="3.1 Revisit Transformer"></a>3.1 Revisit Transformer</h2><ul>
<li>Pixel-BERT采用BERT作为跨模态对齐模块。<strong>BERT是多层双向Transformer encoder,能够对所有输入元素的依赖性建模</strong>。</li>
<li>基础Transformer模块的两个主要操作是self-attention和feed-forward。对于给定的输入$X∈R^{n×d}$,n是元素的数量，d是特征的维度，我们首先计算query Q,key K和value V。</li>
</ul>
<script type="math/tex; mode=display">Q=W_qX,K=W_kX,V=W_vX</script><ul>
<li>$W_q,W_k,W_v$是对应的权重矩阵，我们计算注意力输出$X_{att}$:<script type="math/tex; mode=display">A=softmax(\frac{QK^T}{\sqrt{d}})</script><script type="math/tex; mode=display">X_{att}=AV</script></li>
<li>A是每个输入元素的自注意力权重，我们接着计算输出：<script type="math/tex; mode=display">X_{out}=FFM(X_{att})</script></li>
<li>FFN是带有ReLU激活函数的一组全连接层，上面的操作在所有输入元素间建立了深层连接，包括每个元素本身。在多模态任务上，输入元素来自视觉和语言，我们用Transformer在模态内交互和模态间交互同时建立深层连接。<h2 id="3-2-Model-Architecture"><a href="#3-2-Model-Architecture" class="headerlink" title="3.2 Model Architecture"></a>3.2 Model Architecture</h2><h3 id="Sentence-Feature-Embedding"><a href="#Sentence-Feature-Embedding" class="headerlink" title="Sentence Feature Embedding"></a>Sentence Feature Embedding</h3></li>
<li>给定一段语句作为输入，我们首先将它分为单词序列，使用WordPiece将每个单词分词成token。接着采用embedding矩阵将每个token变成向量。这里我们使用$w=\{w_1,w_2,…w_n\}∈R^d$来表征embeded序列，n是序列的长度，d是embedding维度。同时跟其他基于BERT的语言研究一样加上位置embedding来编码位置信息。最后的表征为序列$\hat{w}_1,\hat{w}_2,…,\hat{w}_n$：<script type="math/tex; mode=display">\hat{w}_i=LayerNorm(w_i+p_i+s_w)</script></li>
<li>$p_i$是位置i的embedding向量，$s_w$是语义embedding向量。<h3 id="Image-Feature-Embedding"><a href="#Image-Feature-Embedding" class="headerlink" title="Image Feature Embedding"></a>Image Feature Embedding</h3></li>
<li>相比使用边界框，我们使用像素来表征区域能够让我们打破形状和边界的限制。像素特征通过CNN visual backbone（如ResNet-50）来学习。对于给定的输入图片I，我们先使用CNN backbone去提取特征，然后沿着空间维度flat特征。我们定义flatten特征为$v=\{v_1,v_2,…v_k\}∈R^d$，k是像素特征的数量。视觉embedding特征$\hat{v}_1,\hat{v}_2,…,\hat{v}_k$可以计算为：<script type="math/tex; mode=display">\hat{v}_i=v_i+s_v</script></li>
<li>$s_v$是区分语言embedding差异的语义embedding向量。<h3 id="Cross-Modality-Module"><a href="#Cross-Modality-Module" class="headerlink" title="Cross-Modality Module"></a>Cross-Modality Module</h3></li>
<li>在获取语句embedding向量和像素特征，我们将所有向量组合去建立输入序列。我们同时加入两个特殊token[CLS]和[SEP]来学习联合分类特征和特定token的长度。（这句话有点绕，我的理解是这两个token就是指明一个模态的开头和结尾）。最后输入到Transformer的序列是：<script type="math/tex; mode=display">\{[CLS],\hat{w}_2,...,\hat{w}_n,[SEP],\hat{v}_1,\hat{v}_2,...,\hat{v}_k\}</script></li>
<li>CNN backbone和Transformer合并成单个模型，是<strong>end-to-end训练</strong>。当我们对Transformer的输出采用学习监督时，梯度可以反向传播到CNN backbone,这样通过CNN backbone学习的特征会更适用于目标学习任务。（就是说CNN的参数可以在预训练和下游任务中backward的，不像之前的OD模型那样freeze住）<h2 id="3-3-Pre-Training"><a href="#3-3-Pre-Training" class="headerlink" title="3.3 Pre-Training"></a>3.3 Pre-Training</h2><h3 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h3></li>
<li>以15%的概率随机mask掉语言token,要求模型基于其他没有被mask的tokens和visual tokens去预测这个maked token。学习目标$L_{MLM}$:<script type="math/tex; mode=display">L_{MLM}(θ)=-E_{(w,I)∈D}logP_θ(w_m|w_{/m}，I)</script></li>
<li>$w_m$是masked token，θ是模型参数，P指似然生成函数。</li>
<li>MLM也是BERT的预训练任务，但是这里不同于它，我们的模型是考虑从语言和视觉tokens来推理出masked token，有助于建立语言模态到视觉模态的映射。<h3 id="Image-Text-Matching"><a href="#Image-Text-Matching" class="headerlink" title="Image-Text Matching"></a>Image-Text Matching</h3></li>
<li>在预训练中，我们在数据集所有图像-语句对中采样，把它们当做积极样例。同时我们也随机shuffle数据集，将没有匹配的图像-语句对当做消极样例。为了防止学习偏差，我们采用相同数量的积极样例和消极样例。</li>
<li>我们在联合embedding特征的[CLS]token上加上一个二分类器来区分图像和语句是否是匹配的。ITM任务的loss function：<script type="math/tex; mode=display">L_{ITM}(θ)=-E_{(w,I)∈D}[ylogS_θ(w,I)+(1-y)log(1-S_θ(w,I))]</script></li>
<li>$y∈\{0,1\}$指文本和图像是否匹配，S是分类得分生成函数。<h3 id="Pixel-Random-Sampling"><a href="#Pixel-Random-Sampling" class="headerlink" title="Pixel Random Sampling"></a>Pixel Random Sampling</h3></li>
<li>为了增强特征学习、防止过拟合，受到dropout的启发，我们在预训练期间随机采样特征像素。在每个迭代过程中，在抽取像素特征后，我们会在它们上随机采样一部分，并将它们喂给Transformer。像素随机采样对于模型训练有两方面的好处：</li>
</ul>
<ol>
<li>鼓励模型从不完整的视觉输入上学习语义知识，进而增强稳健性。</li>
<li>减少输入元素的数量，这样可以减少计算消耗且加速训练进度。</li>
</ol>
<ul>
<li>我们在实验中对于每张图片会从特征图中随机采样一个固定数量=100的像素。</li>
</ul>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4.Experiments"></a>4.Experiments</h1><h2 id="4-1-Pre-training"><a href="#4-1-Pre-training" class="headerlink" title="4.1 Pre-training"></a>4.1 Pre-training</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><ul>
<li>Pixel-BERT在MS-COCO和VG两个大规模图像-文本数据集预训练。利用MSCOCO图像级的caption标注和VG区域级的caption标注作为预训练数据。对于VG数据集，同时采用train和val的数据；对于MSCOCO，将整个数据集分为train,restval，val和test。因为我们的一个下游任务image-text retrieval，是在MSCOCO上进行的，为了防止数据泄露，我们使用train、restval训练。<br><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/3.png" alt="Fig.3"><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3></li>
<li>在预训练中，Pixel-BERT每个迭代上接收一个batch的图像-文本对作为输入，首先使用BERT中WordPiece分词器将每个语句分成语言token;我们在消融分析上使用ResNet-50作为visual backbone，采取更有效的ResNeXt-152去获取更好的表现。我们使用在ImageNet上的预训练模型来初始化visual backbone的权重。在使用ResNet-50作为visual backbone时将输入图像的短边改为800，长边低于1333。在使用ResNeXt-152时，考虑到GPU的内存使用，我们将短边和长边限制在600和1000.CNN visual backbone和Transformer得益于不同的优化器，我们对二者采用不同优化器设置。</li>
</ul>
<ol>
<li>在CNN visual backbone上，采用SGD，学习率1e-2和权重衰减率5e-4.</li>
<li>在Transformer上，采用AdamW,学习率1e-4,权重衰减率1e-2.</li>
</ol>
<ul>
<li>我们用64 NVIDIA Tesla V100 GPU，batch_size=4096个samples,40个peoch来预训练Pixel-BERT。<h2 id="4-2-Downstream-Tasks"><a href="#4-2-Downstream-Tasks" class="headerlink" title="4.2 Downstream Tasks"></a>4.2 Downstream Tasks</h2></li>
<li>我们的模型采用12层Transformer结构作为一个语言模块，我们主要比较其他采用同样Transformer设置的方法。<h3 id="Visual-Question-Answering"><a href="#Visual-Question-Answering" class="headerlink" title="Visual Question Answering"></a>Visual Question Answering</h3></li>
<li>我们将VQA作为一个分类问题，以binary交叉熵损失从[CLS]token学习多层感知。我们以16 NVIDIA Tesla V100 GPU，batch_size=256,18个epoch来微调模型。初始学习率跟预训练一样，然后在第12个和第16个peoch以10倍衰减学习率。<br><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/4.png" alt="Fig.4"><h3 id="Natural-Language-for-Visual-Reasoning-for-Real"><a href="#Natural-Language-for-Visual-Reasoning-for-Real" class="headerlink" title="Natural Language for Visual Reasoning for Real"></a>Natural Language for Visual Reasoning for Real</h3></li>
<li>NLVR任务要求模型去预测一个语言描述是否关联给定的图像对。在我们的模型中，我们将两个图像-语言对放入Pixel-BERT从[CLS]token去取得两个embedding向量,使用它们的concatenation(两个向量接一起)，以交叉熵损失去学习true和false的分类，优化器设置跟VQA的设置一样，但是batch_size=128。<br><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/5.png" alt="Fig.5"><h3 id="Image-Text-Retrieval"><a href="#Image-Text-Retrieval" class="headerlink" title="Image-Text Retrieval"></a>Image-Text Retrieval</h3></li>
<li>我们考虑将检索任务作为一个排名问题。在训练期间，对于每个图像-语句对的图像，我们使用真实的caption作为积极样例，随机采样20个其他对不相关的caption作为消极样例。我们通过所有[CLS]token表征上的一个全连接层来预测得分判断图像和文本是否关联，采取softmax交叉熵损失让积极图像-文本对去取得最高的得分。对于每个图像样例，我们只对取得最高loss的五个消极样例上反向传播梯度。因为图像检索任务与预训练的ITM任务相似，我们只微调Transformer的参数。采用AdamW优化器，1e-4的学习率和1e-2的权重衰减。我们在 8 NVIDIA Tesla GPU，batch_size=64来微调模型。对于Flickr30K，我们训练10个peoch，在第6个epoch衰减学习率；对于MS-COCO，我们训练4个epoch，在第2个epoch衰减学习率。<br><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/6.png" alt="Fig.6"><br><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/7.png" alt="Fig.7"></li>
<li>IR任务取得涨点更多，这是因为IR任务更关注图像的全局描述，我们的模型鼓励模型去学习语言和图片像素间的注意力。<h2 id="4-3-Ablation-Study"><a href="#4-3-Ablation-Study" class="headerlink" title="4.3 Ablation Study"></a>4.3 Ablation Study</h2></li>
<li>预训练模型的表现无法由单一指标衡量，因而我们在下游任务上评估结果。我们可以发现在VQA上，MLM和ITM任务可以带来7.6和1.6的增长。在检索任务上，ITM在TR和IR任务上带来了至少13.0的涨点。NLVR严重依赖于MLM，没有它训练甚至没法收敛。随机采样机制可以在VQA上提升0.5，在NLVR上提升0.4。将visual backbone替换成ResNeXt-152可以大幅度提升表现。<h2 id="4-4-Visualization"><a href="#4-4-Visualization" class="headerlink" title="4.4 Visualization"></a>4.4 Visualization</h2></li>
<li>我们可视化MSCOCO验证集例子attention map的一些中间结果。<br><img src="/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/8.png" alt="Fig.8"><h1 id="5-Conclusion-and-Discussion"><a href="#5-Conclusion-and-Discussion" class="headerlink" title="5 Conclusion and Discussion"></a>5 Conclusion and Discussion</h1></li>
</ul>
<ol>
<li>解决了基于区域的视觉表征的限制。</li>
<li>提出以end-to-end的方法，基于CNN的visual backbone并将其与多模态Transformer结合起来构建Pixel-BERT模型，以像素和文本级别在视觉和语义内容上建立更为准确和周密的embedding。</li>
<li>使用图片的像素作为输入，为了visual embedding的稳健性采取随机像素采样机制。</li>
<li>在VG和MSCOCO数据集上预训练Pixel-BERT来学习广义视觉和语言表征，采取MLM和ITM两个预训练任务，在多数VL任务（VQA,NLVR，IR,TR）上取得SOTA。</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/10/03/My-New-Post/" rel="prev" title="打算开始写一下博客">
                  <i class="fa fa-chevron-left"></i> 打算开始写一下博客
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","cdn":"//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
