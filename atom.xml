<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-10-05T14:33:49.329Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【论文阅读记录01】《Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers》</title>
    <link href="http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/"/>
    <id>http://example.com/2021/10/05/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%9501%E3%80%91%E3%80%8APixel-BERT-Aligning-Image-Pixels-with-Text-by-Deep-Multi-Modal-Transformers%E3%80%8B/</id>
    <published>2021-10-05T02:07:28.000Z</published>
    <updated>2021-10-05T14:33:49.329Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文链接</strong>：<a href="https://arxiv.org/abs/2004.00849">https://arxiv.org/abs/2004.00849</a></p><p><strong>年份</strong>：2020</p><h1 id="0-Abstract"><a href="#0-Abstract" class="headerlink" title="0.Abstract"></a>0.Abstract</h1><ul><li>Pixel-BERT将图像像素与文本对齐，通过深层多模态Transformers在统一的end-to-end框架上来共同学习视觉和语言embedding。</li><li>多数VL任务都使用基于region的图像特征（BUTD啥的），而Pixel-BERT则直接从图像-语句对中建立图像像素和语义更为准确和周密的联系。Pixel-BERT在<strong>像素和文本级别对齐语义连接</strong>,解决了VL任务在特定任务的视觉表征上的限制。另一方面，减轻标注边界框的耗费，克服视觉任务语义标签和语言语义的不平衡。</li><li>在VG和MSCOCO数据集的图像文本对预训练一个end-to-end模型，使用一个随机像素采样的机制来提升视觉特征的鲁棒性，采用MLM和IMT作为预训练任务。</li><li>在VQA和NLVRT等下游任务上取得SOTA，且在VQA任务上单个模型上比当前SOTA涨点2.7。<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1></li><li>多模态预训练模型中，主要使用基于BERT的语言特征和基于region的视觉特征作为输入来共同学习。<blockquote><p>不同模态之间的语义隔阂一直被视为跨模态研究中的挑战。早期VL工作上（VQA或caption)使用在图像分类任务预训练的CNN模型抽取特征。</p></blockquote></li></ul><blockquote><p>再后来，VG数据集和BUTD的出现，使用目标检测模型（如Fast R-CNN）抽取的基于region的视觉特征能取得更好的表现。然而，基于region的视觉特征提取器是专门为如OD任务设计的，这会在语义理解上产生信息隔阂，且边界框的矩形形状在背景中会带来噪声信息，一些重要的视觉信息会丢失，像目标的形状、重叠边界框的对象的空间关系等。此外，特征表征能力还受限于给定种类，目标检测模型中也丢失了有关场景和情绪等更广泛语义的视觉信息。</p></blockquote><p><img src="./img/01/1.png" alt="Fig.1" title="Fig.1"></p><ul><li>在Fig.1中：A例子，让目标检测模型取得飞机的状态较为困难；B例子中，即使可以检测到地面和女孩，但因为她们的region重叠，之后fusion embedding模型对于给定边界框来判断实际空间联系就比较困难；C例子中，视觉特征只有长颈鹿，但很难去推理出这些动物的状态。</li><li><strong>在跨模态联合学习中使用基于region的视觉特征和语言embedding作为Transformer的输入受限在视觉特征所表示的视觉语义</strong>。因此，提出Pixel-BERT来跨出边界框，在VL learning利用图像的所有视觉特征。</li><li>Piexel BERT包含三个部分：<ol><li>visual encoder:一个CNN，将图片像素作为输入。</li><li>language encoder:基于BERT的word-level token embedding。</li><li>fusion model:多模态Transformer</li></ol></li><li><p>为了学习广义特征，首先在图像-语句对数据集上预训练模型。在预训练中采用两个预训练任务和一个预训练机制。在语言上，跟其他预训练工作一样采用MLM；在视觉和语言交互上，采取ITM去区分图像和语句对是否匹配。在视觉上，我们采取<strong>随机像素采样机制</strong>来弥补预测像素级特征的难度。随机像素采样机制提高视觉特征学习的稳健性且克服了过拟合问题。</p></li><li><p><strong>Contributions</strong>（总结一下上面的废话…）:</p></li></ul><ol><li>提出Pixel-BERT——由基于CNN的visualencoder和深层多模态Transformer组成，首先考虑使用自监督学习在像素和文本级别对齐视觉和语言语义。</li><li>我们以预训练方式使用模型，并提出了随机像素采样机制，以提高视觉表现学习的稳健性。</li><li>在包括VQA、Image-Text Retrieval和NLVR的任务上取得SOTA；特别地，我们的方法对比当前SOTA的表现上涨2.17点，比其larger model表现还高。</li></ol><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2.Related Works"></a>2.Related Works</h1><h2 id="2-1-Pre-training-Mechanism"><a href="#2-1-Pre-training-Mechanism" class="headerlink" title="2.1 Pre-training Mechanism"></a>2.1 Pre-training Mechanism</h2><ul><li>基于网络框架，当前多模态预训练的方法分为两类：</li></ul><ol><li>基于Transformer的Two-stream神经网络，two-stream神经网络分别处理视觉和语言信息，然后用另外的Transformer层来融合它们。</li><li>Single-Stream神经网络。在检测边界框特征和语句embedding特征上使用BERT来学习双向联合分布。</li></ol><p>它们的差别在于训练方法、损失函数和数据集。<strong>Pixel-BERT属于Single-Stream，但visual embedding不同于其他方法。</strong></p><h2 id="2-2-Visual-Feature-Embedding-in-Vision-and-Language-Tasks"><a href="#2-2-Visual-Feature-Embedding-in-Vision-and-Language-Tasks" class="headerlink" title="2.2 Visual Feature Embedding in Vision and Language Tasks"></a>2.2 Visual Feature Embedding in Vision and Language Tasks</h2><ul><li>跟第一部分Introdution提到一样，用目标检测模型提取视觉特征的话，视觉语义受限于给定种类，然而语言特征包含更多的语义，造成不平衡。因此，我们将visual encoder学习网络组合成一个框架，并将源图像输入为视觉输入。</li></ul><h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3.Approach"></a>3.Approach</h1><p><img src="./img/01/2.png" alt="Fig.2" title="Fig.2"></p><h2 id="3-1-Revisit-Transformer"><a href="#3-1-Revisit-Transformer" class="headerlink" title="3.1 Revisit Transformer"></a>3.1 Revisit Transformer</h2><ul><li>Pixel-BERT采用BERT作为跨模态对齐模块。<strong>BERT是多层双向Transformer encoder,能够对所有输入元素的依赖性建模</strong>。</li><li>基础Transformer模块的两个主要操作是self-attention和feed-forward。对于给定的输入$X∈R^{n×d}$,n是元素的数量，d是特征的维度，我们首先计算query Q,key K和value V。<script type="math/tex; mode=display">Q=W_qX,K=W_kX,V=W_vX</script></li><li>$W_q,W_k,W_v$是对应的权重矩阵，我们计算注意力输出$X_{att}$:<script type="math/tex; mode=display">A=softmax(\frac{QK^T}{\sqrt{d}})</script><script type="math/tex; mode=display">X_{att}=AV</script></li><li>A是每个输入元素的自注意力权重，我们接着计算输出：<script type="math/tex; mode=display">X_{out}=FFM(X_{att})</script></li><li>FFN是带有ReLU激活函数的一组全连接层，上面的操作在所有输入元素间建立了深层连接，包括每个元素本身。在多模态任务上，输入元素来自视觉和语言，我们用Transformer在模态内交互和模态间交互同时建立深层连接。<h2 id="3-2-Model-Architecture"><a href="#3-2-Model-Architecture" class="headerlink" title="3.2 Model Architecture"></a>3.2 Model Architecture</h2><h3 id="Sentence-Feature-Embedding"><a href="#Sentence-Feature-Embedding" class="headerlink" title="Sentence Feature Embedding"></a>Sentence Feature Embedding</h3></li><li>给定一段语句作为输入，我们首先将它分为单词序列，使用WordPiece将每个单词分词成token。接着采用embedding矩阵将每个token变成向量。这里我们使用$w=\{w_1,w_2,…w_n\}∈R^d$来表征embeded序列，n是序列的长度，d是embedding维度。同时跟其他基于BERT的语言研究一样加上位置embedding来编码位置信息。最后的表征为序列$\hat{w}_1,\hat{w}_2,…,\hat{w}_n$：<script type="math/tex; mode=display">\hat{w}_i=LayerNorm(w_i+p_i+s_w)</script></li><li>$p_i$是位置i的embedding向量，$s_w$是语义embedding向量。<h3 id="Image-Feature-Embedding"><a href="#Image-Feature-Embedding" class="headerlink" title="Image Feature Embedding"></a>Image Feature Embedding</h3></li><li>相比使用边界框，我们使用像素来表征区域能够让我们打破形状和边界的限制。像素特征通过CNN visual backbone（如ResNet-50）来学习。对于给定的输入图片I，我们先使用CNN backbone去提取特征，然后沿着空间维度flat特征。我们定义flatten特征为$v=\{v_1,v_2,…v_k\}∈R^d$，k是像素特征的数量。视觉embedding特征$\hat{v}_1,\hat{v}_2,…,\hat{v}_k$可以计算为：<script type="math/tex; mode=display">\hat{v}_i=v_i+s_v</script></li><li>$s_v$是区分语言embedding差异的语义embedding向量。<h3 id="Cross-Modality-Module"><a href="#Cross-Modality-Module" class="headerlink" title="Cross-Modality Module"></a>Cross-Modality Module</h3></li><li>在获取语句embedding向量和像素特征，我们将所有向量组合去建立输入序列。我们同时加入两个特殊token[CLS]和[SEP]来学习联合分类特征和特定token的长度。（这句话有点绕，我的理解是这两个token就是指明一个模态的开头和结尾）。最后输入到Transformer的序列是：<script type="math/tex; mode=display">\{[CLS],\hat{w}_2,...,\hat{w}_n,[SEP],\hat{v}_1,\hat{v}_2,...,\hat{v}_k\}</script></li><li>CNN backbone和Transformer合并成单个模型，是<strong>end-to-end训练</strong>。当我们对Transformer的输出采用学习监督时，梯度可以反向传播到CNN backbone,这样通过CNN backbone学习的特征会更适用于目标学习任务。（就是说CNN的参数可以在预训练和下游任务中backward的，不像之前的OD模型那样freeze住）<h2 id="3-3-Pre-Training"><a href="#3-3-Pre-Training" class="headerlink" title="3.3 Pre-Training"></a>3.3 Pre-Training</h2><h3 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h3></li><li>以15%的概率随机mask掉语言token,要求模型基于其他没有被mask的tokens和visual tokens去预测这个maked token。学习目标$L_{MLM}$:<script type="math/tex; mode=display">L_{MLM}(θ)=-E_{(w,I)∈D}logP_θ(w_m|w_{/m}，I)</script></li><li>$w_m$是masked token，θ是模型参数，P指似然生成函数。</li><li>MLM也是BERT的预训练任务，但是这里不同于它，我们的模型是考虑从语言和视觉tokens来推理出masked token，有助于建立语言模态到视觉模态的映射。<h3 id="Image-Text-Matching"><a href="#Image-Text-Matching" class="headerlink" title="Image-Text Matching"></a>Image-Text Matching</h3></li><li>在预训练中，我们在数据集所有图像-语句对中采样，把它们当做积极样例。同时我们也随机shuffle数据集，将没有匹配的图像-语句对当做消极样例。为了防止学习偏差，我们采用相同数量的积极样例和消极样例。</li><li>我们在联合embedding特征的[CLS]token上加上一个二分类器来区分图像和语句是否是匹配的。ITM任务的loss function：<script type="math/tex; mode=display">L_{ITM}(θ)=-E_{(w,I)∈D}[ylogS_θ(w,I)+(1-y)log(1-S_θ(w,I))]</script></li><li>$y∈\{0,1\}$指文本和图像是否匹配，S是分类得分生成函数。<h3 id="Pixel-Random-Sampling"><a href="#Pixel-Random-Sampling" class="headerlink" title="Pixel Random Sampling"></a>Pixel Random Sampling</h3></li><li>为了增强特征学习、防止过拟合，受到dropout的启发，我们在预训练期间随机采样特征像素。在每个迭代过程中，在抽取像素特征后，我们会在它们上随机采样一部分，并将它们喂给Transformer。像素随机采样对于模型训练有两方面的好处：</li></ul><ol><li>鼓励模型从不完整的视觉输入上学习语义知识，进而增强稳健性。</li><li>减少输入元素的数量，这样可以减少计算消耗且加速训练进度。</li></ol><ul><li>我们在实验中对于每张图片会从特征图中随机采样一个固定数量=100的像素。</li></ul><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4.Experiments"></a>4.Experiments</h1><h2 id="4-1-Pre-training"><a href="#4-1-Pre-training" class="headerlink" title="4.1 Pre-training"></a>4.1 Pre-training</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><ul><li>Pixel-BERT在MS-COCO和VG两个大规模图像-文本数据集预训练。利用MSCOCO图像级的caption标注和VG区域级的caption标注作为预训练数据。对于VG数据集，同时采用train和val的数据；对于MSCOCO，将整个数据集分为train,restval，val和test。因为我们的一个下游任务image-text retrieval，是在MSCOCO上进行的，为了防止数据泄露，我们使用train、restval训练。<br><img src="./img/01/3.png" alt="Fig.3" title="Fig.3"><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3></li><li>在预训练中，Pixel-BERT每个迭代上接收一个batch的图像-文本对作为输入，首先使用BERT中WordPiece分词器将每个语句分成语言token;我们在消融分析上使用ResNet-50作为visual backbone，采取更有效的ResNeXt-152去获取更好的表现。我们使用在ImageNet上的预训练模型来初始化visual backbone的权重。在使用ResNet-50作为visual backbone时将输入图像的短边改为800，长边低于1333。在使用ResNeXt-152时，考虑到GPU的内存使用，我们将短边和长边限制在600和1000.CNN visual backbone和Transformer得益于不同的优化器，我们对二者采用不同优化器设置。</li></ul><ol><li>在CNN visual backbone上，采用SGD，学习率1e-2和权重衰减率5e-4.</li><li>在Transformer上，采用AdamW,学习率1e-4,权重衰减率1e-2.</li></ol><ul><li>我们用64 NVIDIA Tesla V100 GPU，batch_size=4096个samples,40个peoch来预训练Pixel-BERT。<h2 id="4-2-Downstream-Tasks"><a href="#4-2-Downstream-Tasks" class="headerlink" title="4.2 Downstream Tasks"></a>4.2 Downstream Tasks</h2></li><li>我们的模型采用12层Transformer结构作为一个语言模块，我们主要比较其他采用同样Transformer设置的方法。<h3 id="Visual-Question-Answering"><a href="#Visual-Question-Answering" class="headerlink" title="Visual Question Answering"></a>Visual Question Answering</h3></li><li>我们将VQA作为一个分类问题，以binary交叉熵损失从[CLS]token学习多层感知。我们以16 NVIDIA Tesla V100 GPU，batch_size=256,18个epoch来微调模型。初始学习率跟预训练一样，然后在第12个和第16个peoch以10倍衰减学习率。<br><img src="./img/01/4.png" alt="Fig.4" title="Fig.4"><h3 id="Natural-Language-for-Visual-Reasoning-for-Real"><a href="#Natural-Language-for-Visual-Reasoning-for-Real" class="headerlink" title="Natural Language for Visual Reasoning for Real"></a>Natural Language for Visual Reasoning for Real</h3></li><li>NLVR任务要求模型去预测一个语言描述是否关联给定的图像对。在我们的模型中，我们将两个图像-语言对放入Pixel-BERT从[CLS]token去取得两个embedding向量,使用它们的concatenation(两个向量接一起)，以交叉熵损失去学习true和false的分类，优化器设置跟VQA的设置一样，但是batch_size=128。<br><img src="./img/01/5.png" alt="Fig.5" title="Fig.5"><h3 id="Image-Text-Retrieval"><a href="#Image-Text-Retrieval" class="headerlink" title="Image-Text Retrieval"></a>Image-Text Retrieval</h3></li><li>我们考虑将检索任务作为一个排名问题。在训练期间，对于每个图像-语句对的图像，我们使用真实的caption作为积极样例，随机采样20个其他对不相关的caption作为消极样例。我们通过所有[CLS]token表征上的一个全连接层来预测得分判断图像和文本是否关联，采取softmax交叉熵损失让积极图像-文本对去取得最高的得分。对于每个图像样例，我们只对取得最高loss的五个消极样例上反向传播梯度。因为图像检索任务与预训练的ITM任务相似，我们只微调Transformer的参数。采用AdamW优化器，1e-4的学习率和1e-2的权重衰减。我们在 8 NVIDIA Tesla GPU，batch_size=64来微调模型。对于Flickr30K，我们训练10个peoch，在第6个epoch衰减学习率；对于MS-COCO，我们训练4个epoch，在第2个epoch衰减学习率。<br><img src="./img/01/6.png" alt="Fig.6" title="Fig.6"><br><img src="./img/01/7.png" alt="Fig.7" title="Fig.7"></li><li>IR任务取得涨点更多，这是因为IR任务更关注图像的全局描述，我们的模型鼓励模型去学习语言和图片像素间的注意力。<h2 id="4-3-Ablation-Study"><a href="#4-3-Ablation-Study" class="headerlink" title="4.3 Ablation Study"></a>4.3 Ablation Study</h2></li><li>预训练模型的表现无法由单一指标衡量，因而我们在下游任务上评估结果。我们可以发现在VQA上，MLM和ITM任务可以带来7.6和1.6的增长。在检索任务上，ITM在TR和IR任务上带来了至少13.0的涨点。NLVR严重依赖于MLM，没有它训练甚至没法收敛。随机采样机制可以在VQA上提升0.5，在NLVR上提升0.4。将visual backbone替换成ResNeXt-152可以大幅度提升表现。<h2 id="4-4-Visualization"><a href="#4-4-Visualization" class="headerlink" title="4.4 Visualization"></a>4.4 Visualization</h2></li><li>我们可视化MSCOCO验证集例子attention map的一些中间结果。<br><img src="./img/01/8.png" alt="Fig.8" title="Fig.8"><h1 id="5-Conclusion-and-Discussion"><a href="#5-Conclusion-and-Discussion" class="headerlink" title="5 Conclusion and Discussion"></a>5 Conclusion and Discussion</h1></li></ul><ol><li>解决了基于区域的视觉表征的限制。</li><li>提出以end-to-end的方法，基于CNN的visual backbone并将其与多模态Transformer结合起来构建Pixel-BERT模型，以像素和文本级别在视觉和语义内容上建立更为准确和周密的embedding。</li><li>使用图片的像素作为输入，为了visual embedding的稳健性采取随机像素采样机制。</li><li>在VG和MSCOCO数据集上预训练Pixel-BERT来学习广义视觉和语言表征，采取MLM和ITM两个预训练任务，在多数VL任务（VQA,NLVR，IR,TR）上取得SOTA。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;论文链接&lt;/strong&gt;：&lt;a href=&quot;https://arxiv.org/abs/2004.00849&quot;&gt;https://arxiv.org/abs/2004.00849&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;年份&lt;/strong&gt;：2020&lt;/p&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>打算开始写一下博客</title>
    <link href="http://example.com/2021/10/03/My-New-Post/"/>
    <id>http://example.com/2021/10/03/My-New-Post/</id>
    <published>2021-10-03T02:20:51.000Z</published>
    <updated>2021-10-05T13:47:05.699Z</updated>
    
    <content type="html"><![CDATA[        <h1 id="Hello"   >          <a href="#Hello" class="heading-link"><i class="fas fa-link"></i></a><a href="#Hello" class="headerlink" title="Hello"></a>Hello</h1>      <ul><li>本人本科学经管，因为对计算机感兴趣加上学了要一年的py，所以硕士朝AI方向发展。</li><li>目前是厦门大学人工智能系的研究生，研究方向是VQA、多模态。</li><li>我比较菜，不定期分享一些自己的笔记（不太会看论文、写论文笔记，写得笔记有点像纯粹机翻，但感觉有学进去），如果有写得不好的地方，请各位大佬指点并纠正我一下（email:<span class="exturl"><a class="exturl__link"   href="mailto:&#x73;&#x68;&#117;&#98;&#105;&#x6e;&#x68;&#x75;&#x61;&#x6e;&#x67;&#64;&#115;&#116;&#117;&#x2e;&#120;&#x6d;&#x75;&#x2e;&#x65;&#x64;&#117;&#46;&#99;&#x6e;" >&#x73;&#x68;&#117;&#98;&#105;&#x6e;&#x68;&#x75;&#x61;&#x6e;&#x67;&#64;&#115;&#116;&#117;&#x2e;&#120;&#x6d;&#x75;&#x2e;&#x65;&#x64;&#117;&#46;&#99;&#x6e;</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>）</li><li>还有。。。我虽然笨了点，但我还是有颗想变强的心hh<br><img src="https://img.zcool.cn/community/0130da5d2b3733a80120b5ab7e5b4f.jpg@1280w_1l_2o_100sh.jpg" alt="alt 属性文本"></li></ul>]]></content>
    
    
      
      
    <summary type="html">
        &lt;h1 id=&quot;Hello&quot;   &gt;
          &lt;a href=&quot;#Hello&quot; class=&quot;heading-link&quot;&gt;&lt;i class=&quot;fas fa-link&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;a href=&quot;#Hello&quot; class=&quot;headerlin</summary>
      
    
    
    
    
  </entry>
  
</feed>
